*Service Oriented Architechture
*The communication is through an API
*Amazon EC2: Elastic Compute Cloud
*Properties that need to be configured for Amazon EC2:
 -AMI: Amazon Machine Image
 -Instance Type: Hardware Profile
 -Security Groups
 -Storage
 -Key Pairs
*How my code works?
 -App -> VM -> Server (Host) -> Availability Zone (Logical Data Center) -> Grouping of AZ's in Regions (at least 2 AZ's per Region) -> API Region -> API Call
*Scope of services:
 -AZ scope: EC2
 -Region Scope: S3
*VPC: Virtual Private Cloud
*Instead of configuring each instance for networking the configuration is made using IP segments
*By default the traffic is able to flow between subnets
*If we want that the web servers have direct traffic to Internet we need a Gateway
*The Internet Gateway is defined in "Route Tables"
*VPC has "Region Scope"
*Subnets have "AZ Scope"
*VPC, Subnets and Rules can be imported from a Cloud Information Template
*Port 80 should be closed in an EZ instance if the server is not a Webserver
*Shared Responsibility
 -Developer Responsibility: +Data & Config
							+Application
							+Guest OS
 -AWS Responsibility: 		+Hypervisor
							+Network
							+Physical
*EC Metadata							
 -To view the log file, type the command below in your instance terminal.
  cat /var/log/cloud-init-output.log

 -Explore the log file to see the log entries generated for installing the user data script.
  To view the instance metadata, type the command below:
  curl http://169.254.169.254/latest/meta-data/

 -Execute the command below to get the instance identity document of your instance:
  curl http://169.254.169.254/latest/dynamic/instance-identity/document

 -Execute the command below to get the instance public IP address:
  curl http://169.254.169.254/latest/meta-data/public-ipv4

 -Execute the command below to get the MAC address of the instance:
  curl http://169.254.169.254/latest/meta-data/mac

 -Execute the command below to get the VPC ID in which the instance resides. Make sure to replace Your-MAC in the command below with the MAC address of your instance:
  curl http://169.254.169.254/latest/meta-data/network/interfaces/macs/Your-MAC/vpc-id

 -Execute the command below to get the subnet-id in which the instance resides. Make sure to replace Your-MAC in the command below with the MAC address of your instance:
  curl http://169.254.169.254/latest/meta-data/network/interfaces/macs/Your-MAC/subnet-id

 -Execute the command below to get the instance user data:
  curl http://169.254.169.254/latest/user-data
  
 -169.254.169.254: it's the equivalent to 127.0.0.1 but it's for the EC2 instance making the request

Key Topics
AWS Cloud
The AWS Cloud lets you build applications quickly and cost effectively - you pay for the resources you need and can quickly add more resources when you need them.

Free Tier
You can explore AWS and complete the exercises for this course within the AWS Free Tier. AWS automatically provides alerts using AWS Budgets to help you track your free tier usage. See AWS Free Tier Usage Alerts using AWS Budgets for more.

EC2
Amazon Elastic Compute Cloud allows you to run virtual servers in AWS.
Your virtual server is known as an EC2 Instance. It runs on a physical host that is inside an AWS Availability Zone (AZ). There will be 2 or more AZ within an AWS Region. This design allows you to build applications that are resilient to large scale events that could impact an AZ.

If you'd like to learn more about AWS facilities, take a 'digital tour' of an AWS data center!

VPC
Your network in AWS is provided by Amazon Virtual Private Cloud (VPC). You can create a VPC within an AWS region and within that VPC you define subnets to manage related sets of servers or other AWS resources. VPC lets you define rules for how network traffic from your subnets is routed. You can also decide whether your network should be connected to the Internet, to corporate networks, or to keep the network completely private.

The IP Address ranges for VPC and Subnets are specified using CIDR notation. If you'd like to know more about IP addressing within VPC, see the VPCs and Subnets in the User Guide for Amazon VPC. You can also learn more about CIDR notation in section 3.1 of RFC4632 or in Classless Interdomain Routing on Wikipedia.

Security in AWS
You are given a lot of flexibility in AWS to configure and build your applications the way you want. Given that you control your resources, security in AWS is a shared responsibility between AWS and you. AWS will provide secure facilities and building blocks for your application. AWS also provides guidance, and tools that can help you operate securely.

For example, if you are using EC2, it is your responsibility to take advantage of features such as Security Groups (firewall), Private Subnets (to provide network isolation) and encryption options to build secure applications. You are also responsible for keeping the operating system and application stack patched on your server.

If you use AWS managed services like RDS, you still have to make security decisions, but operational tasks like patching the Operating System and SQL engine can be done automatically on your behalf. When using APIs like Amazon S3 API, the underlying infrastructure and maintenance is fully abstracted from you and you are only responsible for calling the API and configuring your access and encryption policies.

For more on the Shared Security Model, see Shared Responsibility Model on the AWS Compliance site.

Additional Services Used
CloudFormation
An AWS service that can take in a declarative document called a 'template' and use it to provision AWS resources on your behalf so you don't have to. We used this to create a VPC to the specifications needed for the course.

EC2 Metadata service
This is a service that intercepts calls to 169.254.169.254 from your EC2 instance to communicate metadata to the instance. This IP address is in the range for IPv4 Link-Local IP addresses as defined by RFC 3927 and the details about the properties the instance that can be retrieved are documented here in the EC2 User Guide.

What you accomplished this week
You signed up for an AWS Account
You launched your first web server into AWS
You built the virtual network we'll use in upcoming exercises and connected to your EC2 instance

Week 2 Notes

*Everything in AWS is an API called
*BOTO 3 is the SDK for python
*Credentials in AWS are:
 -AWS Access Key
 -AWS Secret Key
*These credentials must be stored in a file called "credentials", we do this through the "aws configure" command, the default output format a JSON
*The credentials determine the permissions
*The authentication is through a service called IAM (Identity and Access Manager)
*IAM can be used with a user that already exists or it can be used to create a new user

*First lines of code:
import boto3
S3 client = boto3.resource('S3'); #it can be also EC3, DynamoDB, and so on
S3.meta.client.upload_file(...here we pass all the parameters needed for the call...)

*When creating a new IAM User:
 -Programmatic access is for users that are intended to work with CLI and SDK'scale
 -AWS Management Console Access is for users that are only intended to manage the system through the console.

*Amazon S3 is object storage built to store and retrieve, any amount of data from anywhere.
*Amazon S3 is designed to provide 99.999999999% of durability and 99.99% of availability.
*Buckets are the fundamental containers in Amazon S3 for data storage, a bucket lives an Amazon Region giving us control over the location of the data.
*From a Bucket Name and an Object Key we can build a URL to access an object:
 -https://%bucket_name%/s3.amazonaws.com/photo/%key%
*By default all S3 Buckets are private, only the AWS Account that created them can access them
*There are 3 methods to set the access control for Buckets:
 -Bucket Policies: JSON-policy document for a bucket which specifies the actions that a principle can take on a bucket
 -IAM for Users and Groups: a policy can be attached to a User or Group and allow them to access to certain buckets
 -Access Control Lists: can be used to grant an AWS Account, list and write privileges to a bucket, or read access to an object
*Signed URL's: The application can share objects with others by creating a pre-signed URL; using the credential in the application, we can grant time-limited permission to view an object. It looks like this:
 -https://%bucket_name%/s3.amazonaws.com/photo/%key%?AWSAccessKeyID=%aKey%&Signature=%secretKey%&Expires=%timestampUnixFormat%

*Amazon Recognition:
 -It's a Machine Learning features
 -Object and Scene recognition
 -Detect-labels: the response is a list of labels with a confident value (the likelihood of being a certain object or scene)
 -Detect-faces: takes an image, and the response is collection of detail structures (bounding box, landmarks, age, eyeglasses, emotions)
 -Facial recognition: is achieved by creating an "index-faces", it's a collection of faces and we can search on it by matching entries from a certain face
  +We can search by faceId
  +We can search by Image
 -Amazon Rekognition has a lot of features like:
  +Find text in images
  +Person tracking
  +Find unsafe content
  +Celebrity recognition
  
*Week 2 Notes

AWS SDKs
AWS SDKs are available for many popular programming languages. The AWS SDK for Python, is called Boto3.

If you would like to install and configure Boto3 locally, review the the Boto3 Quickstart on the Read The Docs site.

SDK documentation will include both high level guides and reference documentation. You can find the documentation for Boto3 at Read the Docs.

AWS Identity and Access Management
AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. We typically use credentials from IAM Users or IAM Roles to authenticate with AWS when making API calls. We control the permissions for which API actions those Users or Roles can perform with IAM Policies.

Making AWS API Calls
AWS API requests are made against a specific API endpoints located in a specific AWS Region. In this class, we are using the "us-west-2" region which is located in Oregon. For example, here are the API endpoints for Rekognition and the API endpoints for S3.

API requests are typically signed with an access key belonging to an IAM User and using the Signature Version 4 Signing Process. It is also possible to sign these API requests using temporary security credentials such as those derived from an IAM Role.

AWS SDKs check several locations for credentials such as local configuration files and environment variables. If the SDK finds credentials, it will automatically sign your API requests for you. For example, here is how the Python SDK checks for Credentials.

Developing in the cloud with AWS Cloud9
AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. You can run this development environment on a managed Amazon EC2 instance that automatically sleeps when you aren't using it.

Make sure to follow the exercise directions carefully when setting up your Cloud9 instance - it needs to be launched in the specified VPC so that you can access resources that you'll be creating in the coming weeks.

Amazon S3
Amazon Simple Storage Service (S3) is object storage built to store and retrieve any amount of data from anywhere – web sites and mobile apps, corporate applications, and data from IoT sensors or devices. You can store files as Objects within S3 Buckets.

In this course, we are using Amazon S3 to store photos.

By default, the objects you put into S3 are private. S3 allows you to use Bucket Policies, IAM Policies, and ACLs to grant permissions to the contents of the bucket. You can also use Presigned URLs for time-limited access to objects.

Presigned URLs are how we are granting access to images in our Python Application. The S3 Service Feature Guide for Boto3 contains Python examples of working with S3. In particular, you should review the sectin on Generating Presigned Urls.

Amazon Rekognition
Amazon Rekognition is a service that applies deep learning to analyze the contents of images and videos. It supports functionality such scene detection, face detection, even celebrity recognition. In this class, we are using the Detect Labels functionality which takes an image as input and returns labels with confidence values such as {Name: lighthouse, Confidence: 98.4629}

Additional Details
More on AWS Identity And Access Management
When you log in to AWS using your email address and password, you are authenticating as the Root User for the account. The best practice is to avoid logging in as Root except for a handful of operations that only the root user can perform.

Instead, you can create IAM Users within your AWS Account for yourself and for any others that need access to resources in your account. IAM Users have a set of permanent credentials such as an Access Key for API access or a Console Password.

Permissions are granted or denied with IAM Policies. By default, all permissions are denied unless explicitly granted. You may select predefined permissions from the list of AWS Managed Policies or define your own custom IAM policies.

If you find that you'll have several users who need similar permissions, you can define an IAM Group and associate your users to the group.

When working with AWS Services, you may also encounter IAM Roles. Many AWS services require that you use roles to control what that service can access. IAM Roles provide only temporary security credentials. One common case is allowing the EC2 service to distribute credentials to your application code running on an EC2 instance. Roles can also enable other scenarios in the enterprise such as cross-account access and identity federation.

AWS Signature V4 Process
If you'd like to explore Signature V4 Signing further, here's some example code that creates HTTP requests and generates the appropriate signature.

What you accomplished this week
You created an IAM User and are following the best practice of not using Root User in your AWS Account
You installed the AWS SDK and configured credentials
You launched and configured Cloud9 IDE environment
You ran a Python Application that makes AWS API calls to Amazon S3 and AWS Rekognition

Week 3 Notes

*IAM Roles are not associated with a specific user or group, instead a trusted identity assumes a role. When we assume a role we receive time-limited security credentials.
*Roles are powered by the AWS Security Token Service.
*Roles can be assumed by:
 -AWS Service, for example an EC2 instance
 -Another AWS Account
*When a role is assumed, BOTO automatically runs with the permission of the role.
*The Temporary Security Credentials for an instance live in the Instance Metadata Service.
*A DB Instance is an isolated database environment in the Cloud.
*A DB instance can be created via:
 -AWS Management Console
 -AWS Command Line Interface
 -RDS(Relational Data Base) API
*A DB Instance is the "major building block" of RDS. 
*A DB Instance can hold or house multiple databases, it's not one database per DB Instance.
*S3 automatically replicates our data, and allows us to turn on things like versioning.
*RDS makes nightly backups of our data, and also to run it in Multi-AZ configuration.
*The latency between AZ's is low enough to sinchronically replicate data, if an application is critical it can also be deployed among multiple AWS Regions.
*AWS Elastic Load Balancer: is a load balancer as a fully managed service and avoids introducing a single point of failure into our application.
*Requirements to configure the load balancer in AWS:
 -Networking configuration (VPC, Security Groups, setting up the ports on which our load balancer is listening, select the AZ's in which we want to be able to load balance the application).
 -Select which EC2 instances it should point the traffic to
 -Configure a health check (the load balancer can check the status of our servers and only direct traffic to server that are able to respond successfully).
 -Start using the load balancer by pointing the hostname to the load  balancer (we can do it using DNS in AWS with route 53, or any other DNS provider).
*uWSGI: Web Server Gateway Interface
*NGINX: is an open source web server

Week 3 Notes and Resources

Key Concepts
Amazon RDS
Amazon Relational Database service allows you to run a managed db instance with your choice of one of 6 relational database engines such as MySQL which we are using in this class. AWS will patch the software on the instance on your behalf and schedule daily backups. You can run your database privately, within your VPC and can also configure it to run in a multi-AZ deployment.

Amazon ELB
Amazon Elastic Load Balancer provides several types of load balancer as a managed service. In this course, we are using Application Load Balancer so that we can isolate users from our application servers. This enables us to operate redundant servers to improve the uptime of our application and in the future, run multiple servers so we can scale our application to handle more users.

Participate
This week, feel free to share you favorite, 'my server crashed and I had to recover it' stories in the discussion forums!

Notes on Hard Drive Failures
For those who are interested, here's a bit more about dice demo.

Probability of rolling '1' six times = (1/6)^6 = 2.14E-05

For the example in the video, we assumed a flat 2% Annualized Failure Rate (AFR) for mechanical hard drives based on publicly available stats from BackBlaze and assumed a constant failure rate over time.

We can find the daily failure rate (p) by modeling the probability that a given hard drive survives through the end of the year.

This leads to the following equation: (1-p)^365 = 0.98

Solving the above for p, we get p = 5.53E-05

We can see that at an AFR of 2%, a hard drive failure is more than twice as likely as rolling the same number 6 times in a row with dice.

The failure rate for hard drives may actually be much higher - industry estimates vary and AWS documentation states commodity drives may fail as often as 4% AFR. Put another way, if you have 1000 hard drives in your data center, by the end of the year you can expect that 20 - 40 of those drives have failed!

In practice, the failure rate varies with time. If you really want to dig into hard drive failures you can review the following papers.

Disk failures in the real world
Flash Reliability in Production
Does the hard drive failure rate apply to my application on EC2?
Not directly as you will typically provision storage volumes for EC2 using Amazon EBS.

"Amazon EBS volumes are designed for an annual failure rate (AFR) of between 0.1% - 0.2%, where failure refers to a complete or partial loss of the volume, depending on the size and performance of the volume. This makes EBS volumes 20 times more reliable than typical commodity disk drives, which fail with an AFR of around 4%.

For example, if you have 1,000 EBS volumes running for 1 year, you should expect 1 to 2 will have a failure. EBS also supports a snapshot feature, which is a good way to take point-in-time backups of your data."

--Amazon EBS Availability and Durability

What you accomplished this week
You learned how to use a managed database provided by Amazon RDS to store the descriptions and tags of the user images.
You also created a load balancer so that you could add redundant copies of your web server to improve the availability and scalability of the application.

Week 4 Notes

*Identity: Cognito User Pools
*Authorization: in handled on the python code that consumes the information from RDS
*Encryption: Protects the Data in Transit and the data at rest.
*Cognito User Pools:
 -Is a directory of our users
 -Attributes: user, email, password, phone
 -Define how the users authenticate
 -What information must be provided to sign up for an account (nickname, other)
 -Policies for password strings and administrative rules
 -After the signup the user has to validate its information before they can sign in
 -The messages for the user verification can be customized
 -For each application that is using the same Cognito User Pool, we can create an app client
*App Clients:
 -It lets us to generate a client secret
 -Our application can authenticate itself into Cognito, and by this way it can interact with the User Pool
 -Here we define our sign-in/sign-out URL's
 -In order to complete our integration with Cognito we'll need to add some parameters from the Cognito User Pool:
  +Cognito User Pool ID
  +Client ID
  +App Secret (Client Secret)
 -We need to configure the Cognito Domain, this will be the place in which Cognito will host all the UI for sign-in/sign-out flows

*Authorization flows:
 -Parameters required by the login:
  +CSRF State
  +Response Type, the format in which we want to receive the information retrieved by Cognito
  +Client ID
  +Redirect URL
  +cognito_login = ("https://%s/login?response_type=code&client_id=%s&redirect_uri=%s", COGNITO_DOMAIN, COGNITO_CLIEND_ID, BASE_URL)
  +cognito_logout = ("https://%s/logout?response_type=code&client_id=%s&logout_uri=%s", COGNITO_DOMAIN, COGNITO_CLIEND_ID, BASE_URL)
  +The Callback URL contains the token that will be exchanged for a Token:
	#http://docs.aws.amazon.com/cognito/latest/developerguide/token-endpoint.html
	csrf_state = request.args.get('state')
	code = request.args.get('code')
	request_parameters = {'grant_type': 'authorization_code',
						'client_id': COGNITO_CLIENT_ID,
						'code': code,
						'redirect_uri': BASE_URL + "/callback"}
	response = request.post("https://%s/oauth2/token" % COGNITO_DOMAIN, 
							data=request_parameters,
							auth=HTTPBasicAuth(COGNITO_CLIENT_ID,
												COGNITO_CLIENT_SECRET))
  +Cognito responds back with some Tokens in JWT format, we need to verify the signatures from these Tokens and also extract some information from them:
	#the response:
	#http://docs.aws.amazon.com/cognito/latest/developerguide/amazon_cognito_user_pools_using_tokens_with_identity_providers
	if response.status_code = requests.codes.ok and csrf == session['csrf_state']:
		id_token = verify(response.json()['id_token'])
		verify(response.json['access_token'])
		
		user = User()
		user.id = id_token['cognito:username']
		session['nickname'] = id_token['nickname']
		session['expires'] = id_token['exp']
		session['refresh_token'] = id_token['refresh_token']
		flask_login.login_user(user, remember=True)
		return redirect(url_for("home"))
  +At the start of the application we need to download some keys that we're going to use validate the "id_token" and the "access_token"
  +Is important to protect persisted resources with some extra validations that ensure that users are not deleting information from other users, this is by validating the user_id
*TLS to Secure the Connection between Client and Server, this is achieved by using a Certificate that must be configured in our Load Balancer, we can use "Amazon Certificate Manager" to do so.
*End to End Encryption:
 -The encryption can be applied to data "in-transit" or even to "data-in-rest"
 -AWS HTTPS API for S3: can be helpful to protect the data in requests made to "Buckets"
 -We'll need to also use HTTPS for the calls made to "Cognito" and "Rekognition"
 -RDS also provides a certificate that can be used to set up an SSL connection to your DB endpoint
 -Amazon Cognito already encripts the information that is "at-rest"
 -Amazon Rekognition isn't storing data so it's not affected 
 -Amazon RDS depending on the DB type we can enable an encryption option, the same goes for Amazon S3 (we can enable encryption for our bucket)
 -Amazon EC2 also has an storage volume that uses EBS (Elastic Block Storage), EBS allow us to encrypt the data "at-rest"
 -By default the encryption key is managed by AWS, or if we want to have more control over these keys we can use Amazon Key Manager
*The AWS Application Load Balancer can be configured along with Amazon Application Firewall, this protects our applications from common attacks like XSS attacks and SQL Injection
*SSH Tunnel to Connect to C9:
 -ssh -i mycert ec-user@%IP_ADDRESS% - L localhost:5000:localhost(remote):5000
 
 
Week 4 Notes and Resources

Key Concepts
Amazon Cognito User Pools
Create and maintain a user directory and add sign-up and sign-in to your mobile app or web application using user pools. User pools scale to hundreds of millions of users and are designed to provide simple, secure, and low-cost options for you as a developer.

You can use user pools to add user registration and sign-in features to your apps. Instead of using external identity providers such as Facebook, or Google, you can use user pools to let users register with or sign in to an app using an email address, phone number, or a user name.

Amazon Cognito User Pools are compliant with SOC 1-3, PCI DSS, ISO 27001, and is HIPAA-BAA eligible.

For more see Amazon Cognito User Pools

AWS Certificate Manager
AWS Certificate Manager (ACM) can help you deploy your SSL/TLS Certificates to your AWS infrastructure such as the Application Load Balancer. If you don't have a certificate, ACM can issue you a certificate as well.

For more see What is AWS Certificate Manager

Securing the connection between your Users and Your Load Balancer
You can create a listener that uses encrypted connections (also known as SSL offload). This feature enables traffic encryption between your load balancer and the clients that initiate SSL or TLS sessions.

For more see HTTPS Listeners for Your Application Load Balancer

Encrypting Data in AWS
Encrypting data in Transit:

Configure Apache Webserver on Amazon Linux to Use SSL/TLS from EC2 User Guide
Since we are using NGINX you may find this link helpful from eff.org helpful as well
Using SSL to Encrypt a Connection to a DB Instance
Encrypting Data at Rest:
* For encrypting storage volumes for your EC2 Instances, see Amazon EBS Encryption

For encrypting your RDS database, see Encrypting Amazon RDS Resources

For encrypting S3 content, see Amazon S3 Default Encryption for S3 Buckets and AWS S3 Encryption.

If you'd like to take more control of the encryption keys with AWS KMS see How AWS Services use AWS KMS

For more on encrypting your data on AWS, see AWS Security Blog on Encryption

AWS Compliance Programs
If you handle sensitive customer data in your applications, please review specific materials at AWS Compliance Programs

You may also learn more about Security in the AWS Cloud by reviewing the AWS Security Whitepaper

Bonus
Using Cognito User Pools SDK to Update User Attributes
Using the SDK for the Cognito Identity Provider, you can call admin_update_user_attributes method to modify user attributes in the user directory.

Resources for Optional Amazon Polly Challenge
Amazon Polly is a service that performs Text-to-Speech.

You may want to review the Boto3 Amazon Polly reference docs.

What you accomplished this week
You created a user directory using Cognito User Pools
You integrated user authentication and authorization into the application
You connected to your application over an SSH tunnel
You learned about options for securing the data in transit and at rest for the application
  
Week 5 Notes

*AWS X-Ray traces requests made by our application, gathers information, and reports this back to the cloud:
 -This gives us some great performance metrics and error reporting in both prod and dev environments
 -Is intended to analyze en debug distributed applications
 -The X-Ray SDK sends the information to the X-Ray Daemon running on the application, and this sends the information the X-Ray API
 -Allows us to create a map of which pieces are talking to other pieces and the latency of these calls.
 -AWS X-Ray Traces:
  +Segments: it's a representation of a request that an application serves.
  +Sub-segments: record calls to AWS Services and Resources made with AWS SDK, calls to internal or external HTTP API's and even queries to SQL DB's, also it can debug or annotate blocks of code inside your application. 
  +Annotations/Metadata: Annotations are simple key/value pairs that are indexed. Metadata are key/value pairs with values of any type including objects and lists that are not indexed but visible for you to debug inside the console.
 -The X-Ray Daemon is initiated using the following command: ./xray
*AWS Lambda is the service where we can run our code without provisioning or managing the service.
 -The image labels are getting created asynchronously (after the image is uploaded, off the webserver, the image is being processed for labels).
*The following services can be integrated with AWS Lambda:
 -API Gateway (public, create, maintain and secure our API's at any scale) the requests coming from the API can be executed by the lambda function
 -DynamoDB streams: we can fire a lambda function when changes happen in a DynamoDB table
 -Kinesis: this is suppose to stream data, we configure lambda functions to process the stream as it arrives.
 -S3: when a new object is stored into a bucket we can execute a lambda function.
 -SNS (Simple Notification Service): is the place for publish and subscribe inside AWS, where we can publish message into a topic. A topic can have multiple suscriptions, HTTP Post, email, and also, a Lambda function.
 -Cognito: we can use lambda functions to customize the workflow happening inside of Cognito.
*Before sending some code to a Lambda Function, we need to package it into a ZIP or JAR file as a deployment package that contains all of our code and dependencies. This can include native binaries, compiled for the Linux 
 execution environment that Lambda uses.
*The logging statements from Lambda are included onto Amazon CloudWatch Logs.
 
 


 